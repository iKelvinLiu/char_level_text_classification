{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-KernelSize CNN & GRU Char-Level Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Core Idea:\n",
    "1. The data is in a deterministic mapping and there is no tokenization,  \n",
    "   so we choose the Char-Level text classification to solve the problem.\n",
    "2. Using multi-size kernel CNN with GRU by comparing several methods from some papers. Reference as below.\n",
    "3. To make the code clean and readable, just putting all the codes in one file.\n",
    "4. To make the training process faster, just using smaller kernel size in CNN and also the training epoch is limited.\n",
    "5. In the experiment stage, tf.saver is used, for the submitted version, the tf.saver is switched off.\n",
    "\n",
    "##### Reference:\n",
    "1. Kim, Y. (2014). Convolutional neural networks for sentence classification. arXiv preprint arXiv:1408.5882.\n",
    "2. Zhang, X., Zhao, J., & LeCun, Y. (2015). Character-level convolutional networks for text classification. \n",
    "   In Advances in neural information processing systems (pp. 649-657).\n",
    "3. Lai, S., Xu, L., Liu, K., & Zhao, J. (2015, January). \n",
    "   Recurrent Convolutional Neural Networks for Text Classification. In AAAI (Vol. 333, pp. 2267-2273)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model Explanation:\n",
    "\n",
    "1. One-hot of document as the input, it has 3 dimensions (batchsize, sentence_length, one_hot_length) \n",
    "2. The first CNN layers, kernel size 7 and 5 were used to extract different n-grams.\n",
    "3. The higher CNN layers extract higher level of abstraction. \n",
    "3. The max-pooling layers were used to down-sampling without overlap, by using stride=3 and k_size = 3\n",
    "4. Batch-Normalization were used in each cnn layers.\n",
    "5. In this work, I chose the GRU instead of CNN flatten, supposed that this helps to add some positional learning of sentence.\n",
    "6. After GRU layer, the drop_out layer was used to decrease overfitting.\n",
    "7. Batch size is 128, validate data set is 1600, \n",
    "   Learning rate is 0.001, Adam_optimizer was used. And for the drop_out layer, is_training parameter is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Print the training performance:\n",
    "Epoch 4 Step 241: Loss: 0.358734, Accuracy: 0.906250  \n",
    "Epoch 4 Step 242: Loss: 0.580555, Accuracy: 0.871795  \n",
    "Epoch 4 Validate Loss: 0.612117, Validate Accuracy: 0.805658  \n",
    "Epoch 5 Step 241: Loss: 0.286239, Accuracy: 0.921875  \n",
    "Epoch 5 Step 242: Loss: 0.487394, Accuracy: 0.897436  \n",
    "Epoch 5 Validate Loss: 0.703344, Validate Accuracy: 0.800123  \n",
    "Epoch 6 Step 241: Loss: 0.108257, Accuracy: 0.953125  \n",
    "Epoch 6 Step 242: Loss: 0.180196, Accuracy: 0.897436  \n",
    "Epoch 6 Validate Loss: 0.690398, Validate Accuracy: 0.799508  \n",
    "Epoch 7 Step 241: Loss: 0.135933, Accuracy: 0.968750  \n",
    "Epoch 7 Step 242: Loss: 0.154102, Accuracy: 0.923077  \n",
    "Epoch 7 Validate Loss: 0.717810, Validate Accuracy: 0.81734  \n",
    "\n",
    "##### Training instructionsï¼š\n",
    "1. Keep adjusting the parameters in model, and after achieving the 0.817 validation accuracy, seems like it works.\n",
    "2. I also put the best checkpoint in the folder, you can use it to train and validate.\n",
    "3. If you want to use the checkpoint, please change \"restore_train_model = False\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\"\"\" global parameters \"\"\"\n",
    "global_batch_size = 128\n",
    "global_learning_rate = 0.001\n",
    "global_validation_switch = True\n",
    "global_training_proportion = 0.95\n",
    "global_epoch_num = 10\n",
    "base_validation_acc = 0.75\n",
    "\n",
    "\"\"\" file sess \"\"\"\n",
    "train_x_file = \"./.txt\"\n",
    "train_y_file = \"./.txt\"\n",
    "test_x_file = \"./.txt\"\n",
    "test_y_file = \"./.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" data file indicators \"\"\"\n",
    "fp_train_x = codecs.open(train_x_file)\n",
    "fp_train_y = codecs.open(train_y_file)\n",
    "fp_test_x = codecs.open(test_x_file)\n",
    "\n",
    "\"\"\" tf saver \"\"\"\n",
    "restore_train_model = True\n",
    "save_train_model = False\n",
    "save_interval = 3\n",
    "checkpoint_dir = \"./saver/\"\n",
    "\n",
    "\"\"\" generate y label list \"\"\"\n",
    "train_y_lines = fp_train_y.readlines()\n",
    "whole_train_y = [int(i.strip()) for i in train_y_lines]\n",
    "whole_train_y = pd.get_dummies(whole_train_y)\n",
    "\n",
    "\"\"\" generate train x \"\"\"\n",
    "train_x_lines = fp_train_x.readlines()\n",
    "test_x_lines = fp_test_x.readlines()\n",
    "global_sample_num = len(train_x_lines)\n",
    "\n",
    "tmp_list = list()\n",
    "for i in train_x_lines:\n",
    "    tmp_list.append(len(i))\n",
    "for i in test_x_lines:\n",
    "    tmp_list.append(len(i))\n",
    "\n",
    "global_max_len = max(tmp_list) + 1\n",
    "print(\"Global_max_sentence_len: \", global_max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using one-hot to represent the char-level vector. Using all-zero vectors to pad the length of sentence into max_length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" generate one_hot embedding \"\"\"\n",
    "alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "char_one_hot = dict()\n",
    "for i in range(len(alphabet)):\n",
    "    letter = [0 for _ in range(len(alphabet))]\n",
    "    letter[i] = 1\n",
    "    char_one_hot[alphabet[i]] = letter\n",
    "char_one_hot[\"#\"] = [0 for _ in range(len(alphabet))]\n",
    "# print(char_one_hot)\n",
    "\n",
    "\"\"\" generate one_hot train set x \"\"\"\n",
    "whole_train_x = list()\n",
    "for i in train_x_lines:\n",
    "    tmp_vector = list()\n",
    "    for j in i.strip():\n",
    "        tmp_vector.append(char_one_hot[j])\n",
    "    for j in range(global_max_len - len(i.strip())):\n",
    "        tmp_vector.append(char_one_hot[\"#\"])\n",
    "    whole_train_x.append(tmp_vector)\n",
    "\n",
    "\"\"\" generate one_hot text set x \"\"\"\n",
    "whole_test_x = list()\n",
    "for i in test_x_lines:\n",
    "    tmp_vector = list()\n",
    "    for j in i.strip():\n",
    "        tmp_vector.append(char_one_hot[j])\n",
    "    for j in range(global_max_len - len(i.strip())):\n",
    "        tmp_vector.append(char_one_hot[\"#\"])\n",
    "    whole_test_x.append(tmp_vector)\n",
    "\n",
    "\"\"\" convert x and y into np\"\"\"\n",
    "whole_train_x = np.asarray(whole_train_x)\n",
    "print(\"Train X shape: \", np.shape(whole_train_x))\n",
    "whole_train_y = np.array(whole_train_y)\n",
    "print(\"Train Y shape: \", np.shape(whole_train_y))\n",
    "whole_test_x = np.asarray(whole_test_x)\n",
    "print(\"Test X shape: \", np.shape(whole_test_x))\n",
    "\n",
    "fp_train_x.close()\n",
    "fp_train_y.close()\n",
    "fp_test_x.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### To help you understand the model layers, adding print of each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch_index(sample_size, batch_size, num_iter=1, is_shuffle=True):\n",
    "    index = list(range(sample_size))\n",
    "    for j in range(num_iter):\n",
    "        if is_shuffle:\n",
    "            np.random.shuffle(index)\n",
    "        for i in range(int(sample_size / batch_size) + (1 if sample_size % batch_size else 0)):\n",
    "            yield index[i * batch_size:(i + 1) * batch_size]\n",
    "\n",
    "\n",
    "def char_cnn_classifier(x, istrain):\n",
    "    with tf.variable_scope('char_cnn_classifier', reuse=tf.AUTO_REUSE):\n",
    "        print(\"Input layer:\", x)\n",
    "\n",
    "        layer_1_k7 = tf.layers.conv1d(x, 64, 7, padding='same', strides=1)\n",
    "        print(\"Conv layer_1_k7:\", layer_1_k7)\n",
    "        layer_1_k7 = tf.layers.batch_normalization(layer_1_k7)\n",
    "\n",
    "        layer_1_k5 = tf.layers.conv1d(x, 64, 5, padding='same', strides=1)\n",
    "        print(\"Conv layer_1_k5:\", layer_1_k5)\n",
    "        layer_1_k5 = tf.layers.batch_normalization(layer_1_k5)\n",
    "\n",
    "        layer_1_concat = tf.concat([layer_1_k7, layer_1_k5], axis=2)\n",
    "        print(\"Concat layer_1:\", layer_1_concat)\n",
    "\n",
    "        x = tf.layers.max_pooling1d(layer_1_concat, 2, 2, padding='valid')\n",
    "        print(\"Pooling layer 1:\", x)\n",
    "\n",
    "        layer_2_k5 = tf.layers.conv1d(x, 128, 5, padding='same', strides=1)\n",
    "        print(\"Conv layer_2_k5:\", layer_2_k5)\n",
    "        layer_2_k5 = tf.layers.batch_normalization(layer_2_k5)\n",
    "\n",
    "        layer_2_k3 = tf.layers.conv1d(x, 128, 3, padding='same', strides=1)\n",
    "        print(\"Conv layer_2_k3:\", layer_2_k3)\n",
    "        layer_2_k3 = tf.layers.batch_normalization(layer_2_k3)\n",
    "\n",
    "        layer_2_concat = tf.concat([layer_2_k5, layer_2_k3], axis=2)\n",
    "        print(\"Concat layer_2:\", layer_2_concat)\n",
    "\n",
    "        x = tf.layers.max_pooling1d(layer_2_concat, 2, 2, padding='valid')\n",
    "        print(\"Pooling layer 2:\", x)\n",
    "\n",
    "        x = tf.layers.conv1d(x, 256, 3, padding='valid', strides=1, activation=tf.nn.relu)\n",
    "        print(\"Conv layer 3:\", x)\n",
    "        x = tf.layers.batch_normalization(x)\n",
    "\n",
    "        x = tf.layers.max_pooling1d(x, 2, 2, padding='valid')\n",
    "        print(\"Pooling layer 3:\", x)\n",
    "\n",
    "        x = tf.layers.conv1d(x, 256, 3, padding='valid', strides=1, activation=tf.nn.relu)\n",
    "        print(\"Conv layer 4:\", x)\n",
    "        x = tf.layers.batch_normalization(x)\n",
    "\n",
    "        x = tf.layers.max_pooling1d(x, 2, 2, padding='valid')\n",
    "        print(\"Pooling layer 4:\", x)\n",
    "\n",
    "        \"\"\" Here we use GRU instead of Flatten CNN\"\"\"\n",
    "        gru_cell = tf.nn.rnn_cell.GRUCell(256)\n",
    "        _, x = tf.nn.dynamic_rnn(gru_cell, x, dtype=np.float32)\n",
    "        x = tf.layers.dropout(x, rate=0.5, training=istrain)\n",
    "\n",
    "        \"\"\" Flatten for CNN, it's the original way of Char-CNNï¼Œhere is not used \"\"\"\n",
    "        # x = tf.layers.flatten(x)\n",
    "        # print(\"Flatten layer 5:\", x)\n",
    "        #\n",
    "        # x = tf.layers.dense(x, 1024)\n",
    "        # x = tf.layers.dropout(x, rate=0.5, training=istrain)\n",
    "        # print(\"Dense layer 6:\", x)\n",
    "        #\n",
    "        # x = tf.layers.dense(x, 1024)\n",
    "        # x = tf.layers.dropout(x, rate=0.5, training=istrain)\n",
    "        # print(\"Dense layer 7:\", x)\n",
    "        #\n",
    "        x = tf.layers.dense(x, 12)\n",
    "        print(\"FC layer:\", x)\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_input_int = tf.placeholder(tf.float32, shape=[None, global_max_len, 26])\n",
    "sentence_label_input = tf.placeholder(tf.float32, shape=[None, 12])\n",
    "is_train_flag = tf.placeholder(tf.bool)\n",
    "\n",
    "dis_out = char_cnn_classifier(sentence_input_int, is_train_flag)\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=dis_out, labels=sentence_label_input))\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(dis_out, 1), tf.argmax(sentence_label_input, 1))\n",
    "accuracy_op = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "disc_train_op = tf.train.AdamOptimizer(learning_rate=global_learning_rate).minimize(loss)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    \"\"\" restore the train saver\"\"\"\n",
    "    if restore_train_model is True:\n",
    "        checkpoint = tf.train.get_checkpoint_state(checkpoint_dir)\n",
    "        if checkpoint and checkpoint.model_checkpoint_path:\n",
    "            print(\"###INFO### Restore train model from\", checkpoint.model_checkpoint_path)\n",
    "            saver.restore(sess, checkpoint.model_checkpoint_path)\n",
    "\n",
    "    if global_validation_switch is False:\n",
    "        global_training_proportion = 1.0\n",
    "    print(\"Global_training_proportion: \", global_training_proportion)\n",
    "\n",
    "    for global_idx in range(global_epoch_num):\n",
    "        iter_tmp = 0\n",
    "        \"\"\" batch training \"\"\"\n",
    "        for batch_index in generate_batch_index(int(global_sample_num*global_training_proportion),\n",
    "                                                global_batch_size, num_iter=1, is_shuffle=True):\n",
    "            batch_x = whole_train_x[batch_index]\n",
    "            batch_y = whole_train_y[batch_index]\n",
    "            batch_y = np.reshape(batch_y, [len(batch_index), 12])\n",
    "            iter_tmp += 1\n",
    "\n",
    "            feed_dict = {sentence_input_int: batch_x, sentence_label_input: batch_y, is_train_flag: True}\n",
    "            _, batch_loss, batch_acc = sess.run([disc_train_op, loss, accuracy_op], feed_dict=feed_dict)\n",
    "            print('Epoch %i Step %i: Loss: %f, Accuracy: %f' % (global_idx, iter_tmp, batch_loss, batch_acc))\n",
    "\n",
    "        if global_validation_switch is True:\n",
    "            \"\"\" epoch validation \"\"\"\n",
    "            batch_x = whole_train_x[int(global_sample_num*global_training_proportion):]\n",
    "            batch_y = whole_train_y[int(global_sample_num*global_training_proportion):]\n",
    "            batch_y = np.reshape(batch_y, [len(batch_y), 12])\n",
    "            print(len(batch_y))\n",
    "            iter_tmp += 1\n",
    "            feed_dict = {sentence_input_int: batch_x, sentence_label_input: batch_y, is_train_flag: False}\n",
    "            validation_loss, validation_acc = sess.run([loss, accuracy_op], feed_dict=feed_dict)\n",
    "            print('Epoch %i Validate Loss: %f, Validate Accuracy: %f' % (global_idx, validation_loss, validation_acc))\n",
    "\n",
    "        \"\"\" save the train model\"\"\"\n",
    "        if (save_train_model is True) and (global_idx % save_interval == 0):\n",
    "            saver.save(sess, checkpoint_dir + 'model_saver.ckpt', global_step=global_idx)\n",
    "\n",
    "        if validation_acc > base_validation_acc:\n",
    "            \"\"\" update the max validation accuracy value \"\"\"\n",
    "            base_validation_acc = validation_acc\n",
    "            \"\"\" predict on test set x and write into ytest.txt \"\"\"\n",
    "            feed_dict = {sentence_input_int: whole_test_x, is_train_flag: False}\n",
    "            predict_test_x = sess.run(dis_out, feed_dict=feed_dict)\n",
    "            predict_test_x = np.argmax(predict_test_x, 1)\n",
    "            print(predict_test_x)\n",
    "            fp_test_y = codecs.open(test_y_file, \"w\")\n",
    "            for i in predict_test_x:\n",
    "                fp_test_y.write(str(i)+\"\\n\")\n",
    "            fp_test_y.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
